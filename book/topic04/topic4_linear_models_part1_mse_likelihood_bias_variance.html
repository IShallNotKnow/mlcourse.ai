

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Topic 4. Linear Classification and Regression &#8212; mlcourse.ai</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://c6.patreon.com/becomePatronButton.bundle.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-CN7ZN59CQB"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-CN7ZN59CQB');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'book/topic04/topic4_linear_models_part1_mse_likelihood_bias_variance';</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Topic 4. Linear Classification and Regression" href="topic4_linear_models_part2_logit_likelihood_learning.html" />
    <link rel="prev" title="Topic 4. Linear Classification and Regression" href="topic04_intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/mlcourse_ai_logo.jpg" class="logo__image only-light" alt="mlcourse.ai - Home"/>
    <script>document.write(`<img src="../../_static/mlcourse_ai_logo.jpg" class="logo__image only-dark" alt="mlcourse.ai - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Intro
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Prerequisites</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../prereqs/python.html">Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prereqs/math.html">Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prereqs/software_devops.html">Software &amp; DevOps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prereqs/docker.html">Docker</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic01/topic01_intro.html">Topic 1 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic01/topic01_pandas_data_analysis.html">Exploratory data analysis with Pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic01/videolecture01.html">Videolecture 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic01/assignment01_pandas_uci_adult.html">Demo Assignment 1 Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic01/assignment01_pandas_uci_adult_solution.html">Demo Assignment 1 Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic01/bonus_assignment01_pandas_olympics.html">Bonus Assignment 1</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic02/topic02_intro.html">Topic 2 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic02/topic02_visual_data_analysis.html">Visual Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic02/topic02_additional_seaborn_matplotlib_plotly.html">Seaborn, Matplotlib, Plotly</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic02/videolecture02.html">Videolecture 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic02/assignment02_analyzing_cardiovascular_desease_data.html">Demo Assignment 2 Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic02/assignment02_analyzing_cardiovascular_desease_data_solution.html">Demo Assignment 2 Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic02/bonus_assignment02_visual_analysis.html">Bonus Assignment 2</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic03/topic03_intro.html">Topic 3 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic03/topic03_decision_trees_kNN.html">Classification, Decision Trees &amp; k Nearest Neighbors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic03/videolecture03.html">Videolecture 3</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic03/assignment03_decision_trees.html">Demo Assignment 3 Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic03/assignment03_decision_trees_solution.html">Demo Assignment 3 Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic03/bonus_assignment03_decision_trees.html">Bonus Assignment 3</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 4</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="topic04_intro.html">Topic 4 Intro</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Ordinary Least Squares</a></li>
<li class="toctree-l1"><a class="reference internal" href="topic4_linear_models_part2_logit_likelihood_learning.html">Linear classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="topic4_linear_models_part3_regul_example.html">An illustrative example of logistic regression regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="topic4_linear_models_part4_good_bad_logit_movie_reviews_XOR.html">When logistic regression is good and when it is not</a></li>
<li class="toctree-l1"><a class="reference internal" href="topic4_linear_models_part5_valid_learning_curves.html">Validation and learning curves</a></li>
<li class="toctree-l1"><a class="reference internal" href="videolecture04.html">Videolecture 4</a></li>
<li class="toctree-l1"><a class="reference internal" href="assignment04_regression_wine.html">Demo Assignment 4 Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="assignment04_regression_wine_solution.html">Demo Assignment 4 Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="bonus_assignment04_alice_baselines.html">Bonus Assignment 4</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic05/topic05_intro.html">Topic 5 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic05/topic5_part1_bagging.html">Bagging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic05/topic5_part2_random_forest.html">Random Forest</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic05/topic5_part3_feature_importance.html">Feature importance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic05/videolecture05.html">Videolecture 5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic05/assignment05_logit_rf_credit_scoring.html">Demo Assignment 5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic05/assignment05_logit_rf_credit_scoring_solution.html">Demo Assignment 5 Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic05/bonus_assignment05_logreg_rf.html">Bonus Assignment 5</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic06/topic06_intro.html">Topic 6 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic06/topic6_feature_engineering_feature_selection.html">Feature engineering &amp; feature selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic06/demo_assignment06.html">Demo Assignment 6 Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic06/bonus_assignment06.html">Bonus Assignment 6</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 7</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic07/topic07_intro.html">Topic 7 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic07/topic7_pca_clustering.html">Unsupervised learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic07/videolecture07.html">Videolecture 7</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic07/assignment07_unsupervised_learning.html">Demo Assignment 7</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic07/assignment07_unsupervised_learning_solution.html">Demo Assignment 7 Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic07/bonus_assignment07.html">Bonus Assignment 7</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 8</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic08/topic08_intro.html">Topic 8 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic08/topic08_sgd_hashing_vowpal_wabbit.html">Vowpal Wabbit: Learning with Gigabytes of Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic08/videolecture08.html">Videolecture 8</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic08/assignment08_implement_sgd_regressor.html">Demo Assignment 8</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic08/assignment08_implement_sgd_regressor_solution.html">Demo Assignment 8 Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic08/bonus_assignment08.html">Bonus Assignment 8</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 9</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic09/topic09_intro.html">Topic 9 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic09/topic9_part1_time_series_python.html">Topic 9. Part 1. Time series analysis in Python</a></li>







<li class="toctree-l1"><a class="reference internal" href="../topic09/topic9_part2_facebook_prophet.html">Predicting the future with Facebook Prophet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic09/videolecture09.html">Videolecture 9</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic09/assignment09_time_series.html">Demo Assignment 9</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic09/assignment09_time_series_solution.html">Demo Assignment 9 Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic09/bonus_assignment09.html">Bonus Assignment 9</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 10</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic10/topic10_intro.html">Topic 10 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic10/topic10_gradient_boosting.html">Gradient boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic10/videolecture10.html">Videolecture 10</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic10/assignment10_flight_delays_kaggle.html">Demo Assignment 10</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic10/bonus_assignment10.html">Bonus Assignment 10</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About the course</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../extra/tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extra/rating.html">Rating</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extra/resources.html">Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extra/contributors.html">Contributors</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/Yorko/mlcourse.ai" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Yorko/mlcourse.ai/edit/main/mlcourse_ai_jupyter_book/book/topic04/topic4_linear_models_part1_mse_likelihood_bias_variance.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Yorko/mlcourse.ai/issues/new?title=Issue%20on%20page%20%2Fbook/topic04/topic4_linear_models_part1_mse_likelihood_bias_variance.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/book/topic04/topic4_linear_models_part1_mse_likelihood_bias_variance.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Topic 4. Linear Classification and Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-ordinary-least-squares">Part 1. Ordinary Least Squares</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#article-outline">Article outline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">2. Maximum Likelihood Estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-variance-decomposition">3. Bias-Variance Decomposition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-of-linear-regression">4. Regularization of Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#useful-resources">5. Useful resources</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="topic-4-linear-classification-and-regression">
<span id="topic04-part1"></span><h1><a class="toc-backref" href="#id1" role="doc-backlink">Topic 4. Linear Classification and Regression</a><a class="headerlink" href="#topic-4-linear-classification-and-regression" title="Permalink to this heading">#</a></h1>
<section id="part-1-ordinary-least-squares">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Part 1. Ordinary Least Squares</a><a class="headerlink" href="#part-1-ordinary-least-squares" title="Permalink to this heading">#</a></h2>
<figure class="align-default">
<img alt="../../_images/ods_stickers.jpg" src="../../_images/ods_stickers.jpg" />
</figure>
<p>Author: <a class="reference external" href="http://pavelnesterov.info/">Pavel Nesterov</a>. Translated and edited by <a class="reference external" href="https://www.linkedin.com/in/christinabutsko/">Christina Butsko</a>, <a class="reference external" href="https://yorko.github.io">Yury Kashnitsky</a>, <a class="reference external" href="https://www.linkedin.com/in/nersesbagiyan/">Nerses Bagiyan</a>, <a class="reference external" href="https://www.linkedin.com/in/yuliya-klimushina-7168a9139">Yulia Klimushina</a>, and <a class="reference external" href="https://www.linkedin.com/in/yuanyuanpao/">Yuanyuan Pao</a>. This material is subject to the terms and conditions of the <a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons CC BY-NC-SA 4.0</a> license. Free use is permitted for any non-commercial purpose.</p>
</section>
<section id="article-outline">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Article outline</a><a class="headerlink" href="#article-outline" title="Permalink to this heading">#</a></h2>
<nav class="contents" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#topic-4-linear-classification-and-regression" id="id1">Topic 4. Linear Classification and Regression</a></p>
<ul>
<li><p><a class="reference internal" href="#part-1-ordinary-least-squares" id="id2">Part 1. Ordinary Least Squares</a></p></li>
<li><p><a class="reference internal" href="#article-outline" id="id3">Article outline</a></p></li>
<li><p><a class="reference internal" href="#introduction" id="id4">1. Introduction</a></p></li>
<li><p><a class="reference internal" href="#maximum-likelihood-estimation" id="id5">2. Maximum Likelihood Estimation</a></p></li>
<li><p><a class="reference internal" href="#bias-variance-decomposition" id="id6">3. Bias-Variance Decomposition</a></p></li>
<li><p><a class="reference internal" href="#regularization-of-linear-regression" id="id7">4. Regularization of Linear Regression</a></p></li>
<li><p><a class="reference internal" href="#useful-resources" id="id8">5. Useful resources</a></p></li>
</ul>
</li>
</ul>
</nav>
</section>
<section id="introduction">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">1. Introduction</a><a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
<p>We will start studying linear models with linear regression. First of all, you must specify a model that relates the dependent variable <span class="math notranslate nohighlight">\(y\)</span> to the explanatory factors (or features); for linear models, the dependency function will take the following form: <span class="math notranslate nohighlight">\(\large y = w_0 + \sum_{i=1}^m w_i x_i\)</span>, where <span class="math notranslate nohighlight">\(m\)</span> is the number of features. If we add a fictitious dimension <span class="math notranslate nohighlight">\(x_0 = 1\)</span> (called <em>bias</em> or <em>intercept</em> term) for each observation, then the linear form can be rewritten in a slightly more compact way by pulling the absolute term <span class="math notranslate nohighlight">\(w_0\)</span> into the sum: <span class="math notranslate nohighlight">\(\large y = \sum_{i=0}^m w_i x_i = \textbf{w}^\text{T} \textbf{x}\)</span>. If we have a matrix of <span class="math notranslate nohighlight">\(n\)</span> observations, where the rows are observations from a data set, we need to add a single column of ones on the left. We define the model as follows:</p>
<div class="math notranslate nohighlight">
\[\large \textbf y = \textbf X \textbf w + \epsilon,\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\textbf w \in \mathbb{R}^{m+1}\)</span> – is a <span class="math notranslate nohighlight">\((m+1) \times 1\)</span> column-vector of the model parameters (in machine learning, these parameters are often referred to as <em>weights</em>);</p></li>
<li><p><span class="math notranslate nohighlight">\(\textbf X \in \mathbb{R}^{n \times (m+1)}\)</span> – is a <span class="math notranslate nohighlight">\(n \times (m+1)\)</span> matrix of observations and their features, (including the fictitious column on the left) with full column <a class="reference external" href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)">rank</a>:  <span class="math notranslate nohighlight">\(\text{rank}\left(\textbf X\right) = m + 1 \)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon \in \mathbb{R}^n\)</span> – is a <span class="math notranslate nohighlight">\(n \times 1\)</span> random column-vector, referred to as <em>error</em> or <em>noise</em>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\textbf y \in \mathbb{R}^n\)</span> – is a <span class="math notranslate nohighlight">\(n \times 1\)</span> column-vector - the dependent (or <em>target</em>) variable.</p></li>
</ul>
<p>We can also write this expression out for each observation</p>
<div class="math notranslate nohighlight">
\[\large y_i = \sum_{j=0}^m w_j X_{ij} + \epsilon_i\]</div>
<p>Will apply the following restrictions to the set of random errors <span class="math notranslate nohighlight">\(\epsilon_i\)</span> (see <a class="reference external" href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">Gauss-Markov</a> theorem for deeper statistical motivation):</p>
<ul class="simple">
<li><p>expectation of all random errors is zero:  <span class="math notranslate nohighlight">\(\forall i: \mathbb{E}\left[\epsilon_i\right] = 0 \)</span>;</p></li>
<li><p>all random errors have the same finite variance, this property is called <a href="https://en.wikipedia.org/wiki/Homoscedasticity">homoscedasticity</a>:  <span class="math notranslate nohighlight">\(\forall i: \text{Var}\left(\epsilon_i\right) = \sigma^2 &lt; \infty \)</span>;</p></li>
<li><p>random errors are uncorrelated:  <span class="math notranslate nohighlight">\(\forall i \neq j: \text{Cov}\left(\epsilon_i, \epsilon_j\right) = 0 \)</span>.</p></li>
</ul>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Estimator">Estimator</a>  <span class="math notranslate nohighlight">\(\widehat{w}_i \)</span> of weights  <span class="math notranslate nohighlight">\(w_i \)</span> is called <em>linear</em> if</p>
<div class="math notranslate nohighlight">
\[\large \widehat{w}_i = \omega_{1i}y_1 + \omega_{2i}y_2 + \cdots + \omega_{ni}y_n,\]</div>
<p>where  <span class="math notranslate nohighlight">\(\forall\ k\ \)</span>, <span class="math notranslate nohighlight">\(\omega_{ki} \)</span>
depend only on the samples from <span class="math notranslate nohighlight">\(\textbf X\)</span>, but not on <span class="math notranslate nohighlight">\(\textbf w\)</span> - we don’t know the true coefficients <span class="math notranslate nohighlight">\(w_i\)</span> (that’s why we try to estimate them with the given data). Since the solution for finding the optimal weights is a linear estimator, the model is called <em>linear regression</em>.</p>
<p><strong>Disclaimer:</strong> at this point, it’s time to say that we don’t plan to squeeze a whole statistics course into one article, so we try to provide the general idea of the math behind OLS.</p>
<p>Let’s introduce one more definition. Estimator <span class="math notranslate nohighlight">\(\widehat{w}_i \)</span> is called <em>unbiased</em> when its expectation is equal to the real but unknown value of the estimated parameter:</p>
<div class="math notranslate nohighlight">
\[\Large \mathbb{E}\left[\widehat{w}_i\right] = w_i\]</div>
<p>One of the ways to calculate those weights is with the ordinary least squares method (OLS), which minimizes the mean squared error between the actual value of the dependent variable and the predicted value given by the model:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Large \begin{array}{rcl}\mathcal{L}\left(\textbf X, \textbf{y}, \textbf{w} \right) &amp;=&amp; \frac{1}{2n} \sum_{i=1}^n \left(y_i - \textbf{w}^\text{T} \textbf{x}_i\right)^2 \\
&amp;=&amp; \frac{1}{2n} \left\| \textbf{y} - \textbf X \textbf{w} \right\|_2^2 \\
&amp;=&amp; \frac{1}{2n} \left(\textbf{y} - \textbf X \textbf{w}\right)^\text{T} \left(\textbf{y} - \textbf X \textbf{w}\right)
\end{array}\end{split}\]</div>
<p>To solve this optimization problem, we need to calculate derivatives with respect to the model parameters. We set them to zero and solve the resulting equation for <span class="math notranslate nohighlight">\(\textbf w\)</span> (matrix differentiation may seem difficult; try to do it in terms of sums to be sure of the answer).</p>
<br>
<details>
<summary>Small CheatSheet on matrix derivatives (click the triangle to extend)</summary>
<p>
<div class="math notranslate nohighlight">
\[\begin{split}\large \begin{array}{rcl}
\frac{\partial}{\partial \textbf{X}} \textbf{X}^{\text{T}} \textbf{A} &amp;=&amp; \textbf{A} \\
\frac{\partial}{\partial \textbf{X}} \textbf{X}^{\text{T}} \textbf{A} \textbf{X} &amp;=&amp; \left(\textbf{A} + \textbf{A}^{\text{T}}\right)\textbf{X} \\
\frac{\partial}{\partial \textbf{A}} \textbf{X}^{\text{T}} \textbf{A} \textbf{y} &amp;=&amp;  \textbf{X}^{\text{T}} \textbf{y}\\
\frac{\partial}{\partial \textbf{X}} \textbf{A}^{-1} &amp;=&amp; -\textbf{A}^{-1} \frac{\partial \textbf{A}}{\partial \textbf{X}} \textbf{A}^{-1}
\end{array}\end{split}\]</div>
</p>
</details>
<p>What we get is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Large \begin{array}{rcl} \frac{\partial \mathcal{L}}{\partial \textbf{w}} &amp;=&amp; \frac{\partial}{\partial \textbf{w}} \frac{1}{2n} \left( \textbf{y}^{\text{T}} \textbf{y} -2\textbf{y}^{\text{T}} \textbf{X} \textbf{w} + \textbf{w}^{\text{T}} \textbf{X}^{\text{T}} \textbf{X} \textbf{w}\right) \\
&amp;=&amp; \frac{1}{2n} \left(-2 \textbf{X}^{\text{T}} \textbf{y} + 2\textbf{X}^{\text{T}} \textbf{X} \textbf{w}\right)
\end{array}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\Large \begin{array}{rcl} \frac{\partial \mathcal{L}}{\partial \textbf{w}} = 0 &amp;\Leftrightarrow&amp; \frac{1}{2n} \left(-2 \textbf{X}^{\text{T}} \textbf{y} + 2\textbf{X}^{\text{T}} \textbf{X} \textbf{w}\right) = 0 \\
&amp;\Leftrightarrow&amp; -\textbf{X}^{\text{T}} \textbf{y} + \textbf{X}^{\text{T}} \textbf{X} \textbf{w} = 0 \\
&amp;\Leftrightarrow&amp; \textbf{X}^{\text{T}} \textbf{X} \textbf{w} = \textbf{X}^{\text{T}} \textbf{y} \\
&amp;\Leftrightarrow&amp; \textbf{w} = \left(\textbf{X}^{\text{T}} \textbf{X}\right)^{-1} \textbf{X}^{\text{T}} \textbf{y}
\end{array}\end{split}\]</div>
<p>Bearing in mind all the definitions and conditions described above, we can say that, based on the <a href="https://en.wikipedia.org/wiki/Gauss–Markov_theorem">Gauss–Markov theorem</a>, OLS estimators of the model parameters are optimal among all linear and unbiased estimators, i.e. they give the lowest variance.</p>
</section>
<section id="maximum-likelihood-estimation">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">2. Maximum Likelihood Estimation</a><a class="headerlink" href="#maximum-likelihood-estimation" title="Permalink to this heading">#</a></h2>
<p>One could ask why we choose to minimize the mean square error instead of something else? After all, one can minimize the mean absolute value of the residual. The only thing that will happen, if we change the minimized value, is that we will exceed the Gauss-Markov theorem conditions, and our estimates will therefore cease to be the optimal over the linear and unbiased ones.</p>
<p>Before we continue, let’s digress to illustrate maximum likelihood estimation with a simple example.</p>
<p>Many people probably remember the formula of ethyl alcohol, so I decided to do an experiment to determine whether people remember a simpler formula for methanol: <span class="math notranslate nohighlight">\(CH_3OH\)</span>. We surveyed 400 people to find that only 117 people remembered the formula. Then, it is reasonable to assume that the probability that the next respondent knows the formula of methyl alcohol is <span class="math notranslate nohighlight">\(\frac{117}{400} \approx 29\%\)</span>. Let’s show that this intuitive assessment is not only good but also a maximum likelihood estimate. Where this estimate come from? Recall the definition of the Bernoulli distribution: a random variable has a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a> if it takes only two values (<span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(0\)</span> with probability <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(1 - \theta\)</span>, respectively) and has the following probability distribution function:</p>
<div class="math notranslate nohighlight">
\[\Large p\left(\theta, x\right) = \theta^x \left(1 - \theta\right)^\left(1 - x\right), x \in \left\{0, 1\right\}\]</div>
<p>This distribution is exactly what we need, and the distribution parameter <span class="math notranslate nohighlight">\(\theta\)</span> is the estimate of the probability that a person knows the formula of methyl alcohol. In our <span class="math notranslate nohighlight">\(400\)</span> <em>independent</em> experiments, let’s denote their outcomes as <span class="math notranslate nohighlight">\(\textbf{x} = \left(x_1, x_2, \ldots, x_{400}\right)\)</span>. Let’s write down the likelihood of our data (observations), i.e. the probability of observing exactly these 117 realizations of the random variable <span class="math notranslate nohighlight">\(x = 1\)</span> and 283 realizations of <span class="math notranslate nohighlight">\(x = 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[\Large p(\textbf{x}; \theta) = \prod_{i=1}^{400} \theta^{x_i} \left(1 - \theta\right)^{\left(1 - x_i\right)} = \theta^{117} \left(1 - \theta\right)^{283}\]</div>
<p>Next, we will maximize the expression with respect to <span class="math notranslate nohighlight">\(\theta\)</span>. Most often this is not done with the likelihood <span class="math notranslate nohighlight">\(p(\textbf{x}; \theta)\)</span> but with its logarithm (the monotonic transformation does not change the solution but simplifies calculation greatly):</p>
<div class="math notranslate nohighlight">
\[\Large \log p(\textbf{x}; \theta) = \log \prod_{i=1}^{400} \theta^{x_i} \left(1 - \theta\right)^{\left(1 - x_i\right)} = \]</div>
<div class="math notranslate nohighlight">
\[ \large = \log \theta^{117} \left(1 - \theta\right)^{283} =  117 \log \theta + 283 \log \left(1 - \theta\right)\]</div>
<p>Now, we want to find such a value of <span class="math notranslate nohighlight">\(\theta\)</span> that will maximize the likelihood. For this purpose, we’ll take the derivative with respect to <span class="math notranslate nohighlight">\(\theta\)</span>, set it to zero, and solve the resulting equation:</p>
<div class="math notranslate nohighlight">
\[\Large  \frac{\partial \log p(\textbf{x}; \theta)}{\partial \theta} = \frac{\partial}{\partial \theta} \left(117 \log \theta + 283 \log \left(1 - \theta\right)\right) = \frac{117}{\theta} - \frac{283}{1 - \theta};\]</div>
<p>It turns out that our intuitive assessment is exactly the maximum likelihood estimate. Now let us apply the same reasoning to the linear regression problem and try to find out what lies beyond the mean squared error. To do this, we’ll need to look at linear regression from a probabilistic perspective. Our model naturally remains the same:</p>
<div class="math notranslate nohighlight">
\[\Large \textbf y = \textbf X \textbf w + \epsilon,\]</div>
<p>but let us now assume that the random errors follow a centered <a class="reference external" href="https://en.wikipedia.org/wiki/Normal_distribution">Normal distribution</a> (beware, it’s an additional assumption, it’s not a prerequisite of a Gauss-Markov theorem):</p>
<div class="math notranslate nohighlight">
\[\Large \epsilon_i \sim \mathcal{N}\left(0, \sigma^2\right)\]</div>
<p>Let’s rewrite the model from a new perspective:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Large \begin{array}{rcl}
y_i &amp;=&amp; \sum_{j=1}^m w_j X_{ij} + \epsilon_i \\
&amp;\sim&amp; \sum_{j=1}^m w_j X_{ij} + \mathcal{N}\left(0, \sigma^2\right) \\
p\left(y_i \mid \textbf X; \textbf{w}\right) &amp;=&amp; \mathcal{N}\left(\sum_{j=1}^m w_j X_{ij}, \sigma^2\right)
\end{array}\end{split}\]</div>
<p>Since the examples are taken independently (uncorrelated errors is one of the conditions of the Gauss-Markov theorem), the full likelihood of the data will look like a product of the density functions <span class="math notranslate nohighlight">\(p\left(y_i\right)\)</span>. Let’s consider the log-likelihood, which allows us to switch products to sums:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Large \begin{array}{rcl}
\log p\left(\textbf{y}\mid \textbf X; \textbf{w}\right) &amp;=&amp; \log \prod_{i=1}^n \mathcal{N}\left(\sum_{j=1}^m w_j X_{ij}, \sigma^2\right) \\
&amp;=&amp; \sum_{i=1}^n \log \mathcal{N}\left(\sum_{j=1}^m w_j X_{ij}, \sigma^2\right) \\
&amp;=&amp; -\frac{n}{2}\log 2\pi\sigma^2 -\frac{1}{2\sigma^2} \sum_{i=1}^n \left(y_i - \textbf{w}^\text{T} \textbf{x}_i\right)^2
\end{array}\end{split}\]</div>
<p>We want to find the maximum likelihood hypothesis i.e. we need to maximize the expression <span class="math notranslate nohighlight">\(p\left(\textbf{y} \mid \textbf X; \textbf{w}\right)\)</span> to get <span class="math notranslate nohighlight">\(\textbf{w}_{\text{ML}}\)</span>, which is the same as maximizing its logarithm. Note that, while maximizing the function over some parameter, you can throw away all the members that do not depend on this parameter:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Large \begin{array}{rcl}
\textbf{w}_{\text{ML}} &amp;=&amp; \arg \max_{\textbf w} p\left(\textbf{y}\mid \textbf X; \textbf{w}\right) = \arg \max_{\textbf w} \log p\left(\textbf{y}\mid \textbf X; \textbf{w}\right)\\
&amp;=&amp; \arg \max_{\textbf w} -\frac{n}{2}\log 2\pi\sigma^2 -\frac{1}{2\sigma^2} \sum_{i=1}^n \left(y_i - \textbf{w}^{\text{T}} \textbf{x}_i\right)^2 \\
&amp;=&amp; \arg \max_{\textbf w} -\frac{1}{2\sigma^2} \sum_{i=1}^n \left(y_i - \textbf{w}^{\text{T}} \textbf{x}_i\right)^2 \\
&amp;=&amp;  \arg \min_{\textbf w} \mathcal{L}\left(\textbf X, \textbf{y}, \textbf{w} \right)
\end{array}\end{split}\]</div>
<p>Thus, we have seen that the maximization of the likelihood of data is the same as the minimization of the mean squared error (given the above assumptions). It turns out that such a cost function is a consequence of the fact that the errors are distributed normally.</p>
</section>
<section id="bias-variance-decomposition">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">3. Bias-Variance Decomposition</a><a class="headerlink" href="#bias-variance-decomposition" title="Permalink to this heading">#</a></h2>
<p>Let’s talk a little about the error properties of linear regression prediction (in fact, this discussion is valid for all machine learning algorithms). We just covered the following:</p>
<ul class="simple">
<li><p>true value of the target variable is the sum of a deterministic function <span class="math notranslate nohighlight">\(f\left(\textbf{x}\right)\)</span> and random error <span class="math notranslate nohighlight">\(\epsilon\)</span>: <span class="math notranslate nohighlight">\(y = f\left(\textbf{x}\right) + \epsilon\)</span>;</p></li>
<li><p>error is normally distributed with zero mean and some variance: <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}\left(0, \sigma^2\right)\)</span>;</p></li>
<li><p>true value of the target variable is also normally distributed: <span class="math notranslate nohighlight">\(y \sim \mathcal{N}\left(f\left(\textbf{x}\right), \sigma^2\right)\)</span>;</p></li>
<li><p>we try to approximate a deterministic but unknown function <span class="math notranslate nohighlight">\(f\left(\textbf{x}\right)\)</span> using a linear function of the covariates <span class="math notranslate nohighlight">\(\widehat{f}\left(\textbf{x}\right)\)</span>, which, in turn, is a point estimate of the function <span class="math notranslate nohighlight">\(f\)</span> in function space (specifically, the family of linear functions that we have limited our space to), i.e. a random variable that has mean and variance.</p></li>
</ul>
<p>So, the error at the point <span class="math notranslate nohighlight">\(\textbf{x}\)</span> decomposes as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Large \begin{array}{rcl}
\text{Err}\left(\textbf{x}\right) &amp;=&amp; \mathbb{E}\left[\left(y - \widehat{f}\left(\textbf{x}\right)\right)^2\right] \\
&amp;=&amp; \mathbb{E}\left[y^2\right] + \mathbb{E}\left[\left(\widehat{f}\left(\textbf{x}\right)\right)^2\right] - 2\mathbb{E}\left[y\widehat{f}\left(\textbf{x}\right)\right] \\
&amp;=&amp; \mathbb{E}\left[y^2\right] + \mathbb{E}\left[\widehat{f}^2\right] - 2\mathbb{E}\left[y\widehat{f}\right] \\
\end{array}\end{split}\]</div>
<p>For clarity, we will omit the notation of the argument of the functions. Let’s consider each member separately. The first two are easily decomposed according to the formula <span class="math notranslate nohighlight">\(\text{Var}\left(z\right) = \mathbb{E}\left[z^2\right] - \mathbb{E}\left[z\right]^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Large \begin{array}{rcl}
\mathbb{E}\left[y^2\right] &amp;=&amp; \text{Var}\left(y\right) + \mathbb{E}\left[y\right]^2 = \sigma^2 + f^2\\
\mathbb{E}\left[\widehat{f}^2\right] &amp;=&amp; \text{Var}\left(\widehat{f}\right) + \mathbb{E}\left[\widehat{f}\right]^2 \\
\end{array}\end{split}\]</div>
<p>Note that:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Large \begin{array}{rcl}
\text{Var}\left(y\right) &amp;=&amp; \mathbb{E}\left[\left(y - \mathbb{E}\left[y\right]\right)^2\right] \\
&amp;=&amp; \mathbb{E}\left[\left(y - f\right)^2\right] \\
&amp;=&amp; \mathbb{E}\left[\left(f + \epsilon - f\right)^2\right] \\
&amp;=&amp; \mathbb{E}\left[\epsilon^2\right] = \sigma^2
\end{array}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\Large \mathbb{E}[y] = \mathbb{E}[f + \epsilon] = \mathbb{E}[f] + \mathbb{E}[\epsilon] = f\]</div>
<p>And finally, we get to the last term in the sum. Recall that the error and the target variable are independent of each other:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Large \begin{array}{rcl}
\mathbb{E}\left[y\widehat{f}\right] &amp;=&amp; \mathbb{E}\left[\left(f + \epsilon\right)\widehat{f}\right] \\
&amp;=&amp; \mathbb{E}\left[f\widehat{f}\right] + \mathbb{E}\left[\epsilon\widehat{f}\right] \\
&amp;=&amp; f\mathbb{E}\left[\widehat{f}\right] + \mathbb{E}\left[\epsilon\right] \mathbb{E}\left[\widehat{f}\right]  = f\mathbb{E}\left[\widehat{f}\right]
\end{array}\end{split}\]</div>
<p>Finally, let’s bring this all together:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Large \begin{array}{rcl}
\text{Err}\left(\textbf{x}\right) &amp;=&amp; \mathbb{E}\left[\left(y - \widehat{f}\left(\textbf{x}\right)\right)^2\right] \\
&amp;=&amp; \sigma^2 + f^2 + \text{Var}\left(\widehat{f}\right) + \mathbb{E}\left[\widehat{f}\right]^2 - 2f\mathbb{E}\left[\widehat{f}\right] \\
&amp;=&amp; \left(f - \mathbb{E}\left[\widehat{f}\right]\right)^2 + \text{Var}\left(\widehat{f}\right) + \sigma^2 \\
&amp;=&amp; \text{Bias}\left(\widehat{f}\right)^2 + \text{Var}\left(\widehat{f}\right) + \sigma^2
\end{array}\end{split}\]</div>
<p>With that, we have reached our ultimate goal – the last formula tells us that the forecast error of any model of type <span class="math notranslate nohighlight">\(y = f\left(\textbf{x}\right) + \epsilon\)</span> is composed of:</p>
<ul class="simple">
<li><p>squared bias: <span class="math notranslate nohighlight">\(\text{Bias}\left(\widehat{f}\right)\)</span> is the average error for all sets of data;</p></li>
<li><p>variance: <span class="math notranslate nohighlight">\(\text{Var}\left(\widehat{f}\right)\)</span> is error variability, or by how much error will vary if we train the model on different sets of data;</p></li>
<li><p>irremovable error: <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p></li>
</ul>
<p>While we cannot do anything with the <span class="math notranslate nohighlight">\(\sigma^2\)</span> term, we can influence the first two. Ideally, we’d like to negate both of these terms (upper left square of the picture), but, in practice, it is often necessary to balance between the biased and unstable estimates (high variance).</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/topic4_bias_variance.png"><img alt="../../_images/topic4_bias_variance.png" src="../../_images/topic4_bias_variance.png" style="width: 480px;" /></a>
</figure>
<p>Generally, as the model becomes more computational (e.g. when the number of free parameters grows), the variance (dispersion) of the estimate also increases, but bias decreases. Due to the fact that the training set is memorized completely instead of generalizing, small changes lead to unexpected results (overfitting). On the other side, if the model is too weak, it will not be able to learn the pattern, resulting in learning something different that is offset with respect to the right solution.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/topic4_bias_variance2.png"><img alt="../../_images/topic4_bias_variance2.png" src="../../_images/topic4_bias_variance2.png" style="width: 480px;" /></a>
</figure>
<p>The Gauss-Markov theorem asserts that the OLS estimator of parameters of the linear model is the best for the class of linear unbiased estimator. This means that if there exists any other unbiased model <span class="math notranslate nohighlight">\(g\)</span>, from the same class of linear models, we can be sure that <span class="math notranslate nohighlight">\(Var\left(\widehat{f}\right) \leq Var\left(g\right)\)</span>.</p>
</section>
<section id="regularization-of-linear-regression">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">4. Regularization of Linear Regression</a><a class="headerlink" href="#regularization-of-linear-regression" title="Permalink to this heading">#</a></h2>
<p>There are situations where we might intentionally increase the bias of the model for the sake of stability i.e. to reduce the variance of the model <span class="math notranslate nohighlight">\(\text{Var}\left(\widehat{f}\right)\)</span>. One of the conditions of the Gauss-Markov theorem is the full column rank of matrix <span class="math notranslate nohighlight">\(\textbf{X}\)</span>. Otherwise, the OLS solution <span class="math notranslate nohighlight">\(\textbf{w} = \left(\textbf{X}^\text{T} \textbf{X}\right)^{-1} \textbf{X}^\text{T} \textbf{y}\)</span> does not exist since the inverse matrix <span class="math notranslate nohighlight">\(\left(\textbf{X}^\text{T} \textbf{X}\right)^{-1}\)</span> does not exist. In other words, matrix <span class="math notranslate nohighlight">\(\textbf{X}^\text{T} \textbf{X}\)</span> will be singular or degenerate. This problem is called an <a href="https://en.wikipedia.org/wiki/Well-posed_problem"> ill-posed problem</a>. Problems like this must be corrected, namely, matrix <span class="math notranslate nohighlight">\(\textbf{X}^\text{T} \textbf{X}\)</span> needs to become non-degenerate, or regular (which is why this process is called regularization). Often we observe the so-called multicollinearity in the data: when two or more features are strongly correlated, it is manifested in the matrix <span class="math notranslate nohighlight">\(\textbf{X}\)</span> in the form of “almost” linear dependence between the columns. For example, in the problem of predicting house prices by their parameters, attributes “area with balcony” and “area without balcony” will have an “almost” linear relationship. Formally, matrix <span class="math notranslate nohighlight">\(\textbf{X}^\text{T} \textbf{X}\)</span> for such data is reversible, but, due to multicollinearity, some of its eigenvalues will be close to zero. In the inverse matrix <span class="math notranslate nohighlight">\(\textbf{X}^\text{T} \textbf{X}\)</span>, some extremely large eigenvalues will appear, as eigenvalues of the inverse matrix are <span class="math notranslate nohighlight">\(\frac{1}{\lambda_i}\)</span>. The result of this vacillation of eigenvalues is an unstable estimate of model parameters, i.e. adding a new set of observations to the training data will lead to a completely different solution.
One method of regularization is <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">Tikhonov regularization</a>, which generally looks like the addition of a new member to the mean squared error:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\Large \begin{array}{rcl}
\mathcal{L}\left(\textbf{X}, \textbf{y}, \textbf{w} \right) &amp;=&amp; \frac{1}{2n} \left\| \textbf{y} - \textbf{X} \textbf{w} \right\|_2^2 + \left\| \Gamma \textbf{w}\right\|^2\\
\end{array}\end{split}\]</div>
<p>The Tikhonov matrix is often expressed as the product of a number by the identity matrix: <span class="math notranslate nohighlight">\(\Gamma = \frac{\lambda}{2} E\)</span>. In this case, the problem of minimizing the mean squared error becomes a problem with a restriction on the <span class="math notranslate nohighlight">\(L_2\)</span> norm. If we differentiate the new cost function with respect to the model parameters, set the resulting function to zero, and rearrange for  <span class="math notranslate nohighlight">\(\textbf{w}\)</span>, we get the exact solution of the problem.</p>
<div class="math notranslate nohighlight">
\[\Large \begin{array}{rcl}
\textbf{w} &amp;=&amp; \left(\textbf{X}^{\text{T}} \textbf{X} + \lambda \textbf{E}\right)^{-1} \textbf{X}^{\text{T}} \textbf{y}
\end{array}\]</div>
<p>This type of regression is called ridge regression. The ridge is the diagonal matrix that we add to the <span class="math notranslate nohighlight">\(\textbf{X}^\text{T} \textbf{X}\)</span> matrix to ensure that we get a regular matrix as a result.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/topic4_ridge.png"><img alt="../../_images/topic4_ridge.png" src="../../_images/topic4_ridge.png" style="width: 480px;" /></a>
</figure>
<p>Such a solution reduces dispersion but becomes biased because the norm of the vector of parameters is also minimized, which makes the solution shift towards zero. On the figure below, the OLS solution is at the intersection of the white dotted lines. Blue dots represent different solutions of ridge regression. It can be seen that by increasing the regularization parameter <span class="math notranslate nohighlight">\(\lambda\)</span>, we shift the solution towards zero.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/topic4_l2_regul.png"><img alt="../../_images/topic4_l2_regul.png" src="../../_images/topic4_l2_regul.png" style="width: 480px;" /></a>
</figure>
</section>
<section id="useful-resources">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">5. Useful resources</a><a class="headerlink" href="#useful-resources" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Medium <a class="reference external" href="https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220">“story”</a> based on this notebook</p></li>
<li><p>Main course <a class="reference external" href="https://mlcourse.ai">site</a>, <a class="reference external" href="https://github.com/Yorko/mlcourse.ai">course repo</a>, and YouTube <a class="reference external" href="https://www.youtube.com/watch?v=QKTuw4PNOsU&amp;list=PLVlY_7IJCMJeRfZ68eVfEcu-UcN9BbwiX">channel</a></p></li>
<li><p>Course materials as a <a class="reference external" href="https://www.kaggle.com/kashnitsky/mlcourse">Kaggle Dataset</a></p></li>
<li><p>If you read Russian: an <a class="reference external" href="https://habrahabr.ru/company/ods/blog/323890/">article</a> on Habrahabr with ~ the same material. And a <a class="reference external" href="https://youtu.be/oTXGQ-_oqvI">lecture</a> on YouTube</p></li>
<li><p>A nice and concise overview of linear models is given in the book <a class="reference external" href="http://www.deeplearningbook.org">“Deep Learning”</a> (I. Goodfellow, Y. Bengio, and A. Courville).</p></li>
<li><p>Linear models are covered practically in every ML book. We recommend “Pattern Recognition and Machine Learning” (C. Bishop) and “Machine Learning: A Probabilistic Perspective” (K. Murphy).</p></li>
<li><p>If you prefer a thorough overview of linear model from a statistician’s viewpoint, then look at “The elements of statistical learning” (T. Hastie, R. Tibshirani, and J. Friedman).</p></li>
<li><p>The book “Machine Learning in Action” (P. Harrington) will walk you through implementations of classic ML algorithms in pure Python.</p></li>
<li><p><a class="reference external" href="http://scikit-learn.org/stable/documentation.html">Scikit-learn</a> library. These guys work hard on writing really clear documentation.</p></li>
<li><p>Scipy 2017 <a class="reference external" href="https://github.com/amueller/scipy-2017-sklearn">scikit-learn tutorial</a> by Alex Gramfort and Andreas Mueller.</p></li>
<li><p>One more <a class="reference external" href="https://github.com/diefimov/MTH594_MachineLearning">ML course</a> with very good materials.</p></li>
<li><p><a class="reference external" href="https://github.com/rushter/MLAlgorithms">Implementations</a> of many ML algorithms. Search for linear regression and logistic regression.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./book/topic04"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="topic04_intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Topic 4. Linear Classification and Regression</p>
      </div>
    </a>
    <a class="right-next"
       href="topic4_linear_models_part2_logit_likelihood_learning.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Topic 4. Linear Classification and Regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-ordinary-least-squares">Part 1. Ordinary Least Squares</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#article-outline">Article outline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">2. Maximum Likelihood Estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-variance-decomposition">3. Bias-Variance Decomposition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-of-linear-regression">4. Regularization of Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#useful-resources">5. Useful resources</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yury Kashnitsky (yorko)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>