
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Topic 4. Linear Classification and Regression &#8212; mlcourse.ai</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-CN7ZN59CQB"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-CN7ZN59CQB');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-CN7ZN59CQB');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'book/topic04/topic4_linear_models_part2_logit_likelihood_learning';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://c6.patreon.com/becomePatronButton.bundle.js"></script>
    <link rel="icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Topic 4. Linear Classification and Regression" href="topic4_linear_models_part3_regul_example.html" />
    <link rel="prev" title="Topic 4. Linear Classification and Regression" href="topic4_linear_models_part1_mse_likelihood_bias_variance.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/mlcourse_ai_logo.jpg" class="logo__image only-light" alt="mlcourse.ai - Home"/>
    <script>document.write(`<img src="../../_static/mlcourse_ai_logo.jpg" class="logo__image only-dark" alt="mlcourse.ai - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Intro
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Prerequisites</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../prereqs/python.html">Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prereqs/math.html">Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prereqs/software_devops.html">Software &amp; DevOps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prereqs/docker.html">Docker</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic01/topic01_intro.html">Topic 1 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic01/topic01_pandas_data_analysis.html">Exploratory data analysis with Pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic01/videolecture01.html">Videolecture 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic01/assignment01_pandas_uci_adult.html">Demo Assignment 1 Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic01/assignment01_pandas_uci_adult_solution.html">Demo Assignment 1 Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic01/bonus_assignment01_pandas_olympics.html">Bonus Assignment 1</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic02/topic02_intro.html">Topic 2 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic02/topic02_visual_data_analysis.html">Visual Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic02/topic02_additional_seaborn_matplotlib_plotly.html">Seaborn, Matplotlib, Plotly</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic02/videolecture02.html">Videolecture 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic02/assignment02_analyzing_cardiovascular_desease_data.html">Demo Assignment 2 Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic02/assignment02_analyzing_cardiovascular_desease_data_solution.html">Demo Assignment 2 Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic02/bonus_assignment02_visual_analysis.html">Bonus Assignment 2</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic03/topic03_intro.html">Topic 3 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic03/topic03_decision_trees_kNN.html">Classification, Decision Trees &amp; k Nearest Neighbors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic03/videolecture03.html">Videolecture 3</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic03/assignment03_decision_trees.html">Demo Assignment 3 Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic03/assignment03_decision_trees_solution.html">Demo Assignment 3 Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic03/bonus_assignment03_decision_trees.html">Bonus Assignment 3</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 4</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="topic04_intro.html">Topic 4 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="topic4_linear_models_part1_mse_likelihood_bias_variance.html">Ordinary Least Squares</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Linear classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="topic4_linear_models_part3_regul_example.html">An illustrative example of logistic regression regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="topic4_linear_models_part4_good_bad_logit_movie_reviews_XOR.html">When logistic regression is good and when it is not</a></li>
<li class="toctree-l1"><a class="reference internal" href="topic4_linear_models_part5_valid_learning_curves.html">Validation and learning curves</a></li>
<li class="toctree-l1"><a class="reference internal" href="videolecture04.html">Videolecture 4</a></li>
<li class="toctree-l1"><a class="reference internal" href="assignment04_regression_wine.html">Demo Assignment 4 Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="assignment04_regression_wine_solution.html">Demo Assignment 4 Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="bonus_assignment04_alice_baselines.html">Bonus Assignment 4</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic05/topic05_intro.html">Topic 5 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic05/topic5_part1_bagging.html">Bagging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic05/topic5_part2_random_forest.html">Random Forest</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic05/topic5_part3_feature_importance.html">Feature importance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic05/videolecture05.html">Videolecture 5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic05/assignment05_logit_rf_credit_scoring.html">Demo Assignment 5</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic05/assignment05_logit_rf_credit_scoring_solution.html">Demo Assignment 5 Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic05/bonus_assignment05_logreg_rf.html">Bonus Assignment 5</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic06/topic06_intro.html">Topic 6 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic06/topic6_feature_engineering_feature_selection.html">Feature engineering &amp; feature selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic06/demo_assignment06.html">Demo Assignment 6 Task</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic06/bonus_assignment06.html">Bonus Assignment 6</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 7</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic07/topic07_intro.html">Topic 7 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic07/topic7_pca_clustering.html">Unsupervised learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic07/videolecture07.html">Videolecture 7</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic07/assignment07_unsupervised_learning.html">Demo Assignment 7</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic07/assignment07_unsupervised_learning_solution.html">Demo Assignment 7 Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic07/bonus_assignment07.html">Bonus Assignment 7</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 8</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic08/topic08_intro.html">Topic 8 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic08/topic08_sgd_hashing_vowpal_wabbit.html">Vowpal Wabbit: Learning with Gigabytes of Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic08/videolecture08.html">Videolecture 8</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic08/assignment08_implement_sgd_regressor.html">Demo Assignment 8</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic08/assignment08_implement_sgd_regressor_solution.html">Demo Assignment 8 Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic08/bonus_assignment08.html">Bonus Assignment 8</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 9</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic09/topic09_intro.html">Topic 9 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic09/topic9_part1_time_series_python.html">Topic 9. Part 1. Time series analysis in Python</a></li>







<li class="toctree-l1"><a class="reference internal" href="../topic09/topic9_part2_facebook_prophet.html">Predicting the future with Facebook Prophet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic09/videolecture09.html">Videolecture 9</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic09/assignment09_time_series.html">Demo Assignment 9</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic09/assignment09_time_series_solution.html">Demo Assignment 9 Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic09/bonus_assignment09.html">Bonus Assignment 9</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topic 10</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../topic10/topic10_intro.html">Topic 10 Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic10/topic10_gradient_boosting.html">Gradient boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic10/videolecture10.html">Videolecture 10</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic10/assignment10_flight_delays_kaggle.html">Demo Assignment 10</a></li>
<li class="toctree-l1"><a class="reference internal" href="../topic10/bonus_assignment10.html">Bonus Assignment 10</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About the course</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../extra/tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extra/rating.html">Rating</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extra/resources.html">Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extra/contributors.html">Contributors</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/Yorko/mlcourse.ai" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Yorko/mlcourse.ai/edit/main/mlcourse_ai_jupyter_book/book/topic04/topic4_linear_models_part2_logit_likelihood_learning.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Yorko/mlcourse.ai/issues/new?title=Issue%20on%20page%20%2Fbook/topic04/topic4_linear_models_part2_logit_likelihood_learning.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/book/topic04/topic4_linear_models_part2_logit_likelihood_learning.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Topic 4. Linear Classification and Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-linear-classification">Part 2. Linear Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#article-outline">Article outline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-classifier">1. Linear Classifier</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-as-a-linear-classifier">2. Logistic Regression as a Linear Classifier</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-and-logistic-regression">3. Maximum Likelihood Estimation and Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-2-regularization-of-logistic-loss">4. <span class="math notranslate nohighlight">\(L_2\)</span>-Regularization of Logistic Loss</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#useful-resources">5. Useful resources</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="topic-4-linear-classification-and-regression">
<span id="topic04-part2"></span><h1><a class="toc-backref" href="#id1" role="doc-backlink">Topic 4. Linear Classification and Regression</a><a class="headerlink" href="#topic-4-linear-classification-and-regression" title="Link to this heading">#</a></h1>
<section id="part-2-linear-classification">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Part 2. Linear Classification</a><a class="headerlink" href="#part-2-linear-classification" title="Link to this heading">#</a></h2>
<figure class="align-default">
<img alt="../../_images/ods_stickers.jpg" src="../../_images/ods_stickers.jpg" />
</figure>
<p><strong><center><a class="reference external" href="https://mlcourse.ai">mlcourse.ai</a> – Open Machine Learning Course</strong> </center><br></p>
<p>Author: <a class="reference external" href="https://yorko.github.io">Yury Kashnitsky</a>. Translated and edited by <a class="reference external" href="https://www.linkedin.com/in/christinabutsko/">Christina Butsko</a>, <a class="reference external" href="https://www.linkedin.com/in/nersesbagiyan/">Nerses Bagiyan</a>, <a class="reference external" href="https://www.linkedin.com/in/yuliya-klimushina-7168a9139">Yulia Klimushina</a>, and <a class="reference external" href="https://www.linkedin.com/in/yuanyuanpao/">Yuanyuan Pao</a>. This material is subject to the terms and conditions of the <a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons CC BY-NC-SA 4.0</a> license. Free use is permitted for any non-commercial purpose.</p>
</section>
<section id="article-outline">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Article outline</a><a class="headerlink" href="#article-outline" title="Link to this heading">#</a></h2>
<nav class="contents" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#topic-4-linear-classification-and-regression" id="id1">Topic 4. Linear Classification and Regression</a></p>
<ul>
<li><p><a class="reference internal" href="#part-2-linear-classification" id="id2">Part 2. Linear Classification</a></p></li>
<li><p><a class="reference internal" href="#article-outline" id="id3">Article outline</a></p></li>
<li><p><a class="reference internal" href="#linear-classifier" id="id4">1. Linear Classifier</a></p></li>
<li><p><a class="reference internal" href="#logistic-regression-as-a-linear-classifier" id="id5">2. Logistic Regression as a Linear Classifier</a></p></li>
<li><p><a class="reference internal" href="#maximum-likelihood-estimation-and-logistic-regression" id="id6">3. Maximum Likelihood Estimation and Logistic Regression</a></p></li>
<li><p><a class="reference internal" href="#l-2-regularization-of-logistic-loss" id="id7">4. <span class="math notranslate nohighlight">\(L_2\)</span>-Regularization of Logistic Loss</a></p></li>
<li><p><a class="reference internal" href="#useful-resources" id="id8">5. Useful resources</a></p></li>
</ul>
</li>
</ul>
</nav>
</section>
<section id="linear-classifier">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">1. Linear Classifier</a><a class="headerlink" href="#linear-classifier" title="Link to this heading">#</a></h2>
<p>The basic idea behind a linear classifier two target classes can be separated by a hyperplane in the feature space. If this can be done without error, the training set is called <em>linearly separable</em>.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/topic4_linear_classifier.png"><img alt="../../_images/topic4_linear_classifier.png" src="../../_images/topic4_linear_classifier.png" style="width: 480px;" />
</a>
</figure>
<p>We have already seen linear regression and Ordinary Least Squares (OLS). Let’s consider a binary classification problem, and denote target classes to be “+1” (positive examples) and “-1” (negative examples). One of the simplest linear classifiers can be defined using regression as follows:</p>
<div class="math notranslate nohighlight">
\[\Large a(\textbf{x}) = \text{sign}(\textbf{w}^\text{T}\textbf x),\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\textbf{x}\)</span> –  is a feature vector (along with identity);</p></li>
<li><p><span class="math notranslate nohighlight">\(\textbf{w}\)</span> – is a vector of weights in the linear model (with bias <span class="math notranslate nohighlight">\(w_0\)</span>);</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{sign}(\bullet)\)</span> – is the signum function that returns the sign of its argument;</p></li>
<li><p><span class="math notranslate nohighlight">\(a(\textbf{x})\)</span> – is a classifier response for <span class="math notranslate nohighlight">\(\textbf{x}\)</span>.</p></li>
</ul>
</section>
<section id="logistic-regression-as-a-linear-classifier">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">2. Logistic Regression as a Linear Classifier</a><a class="headerlink" href="#logistic-regression-as-a-linear-classifier" title="Link to this heading">#</a></h2>
<p>Logistic regression is a special case of the linear classifier, but it has an added benefit of predicting a probability <span class="math notranslate nohighlight">\(p_+\)</span> of referring example <span class="math notranslate nohighlight">\(\textbf{x}_\text{i}\)</span> to the class “+”:</p>
<div class="math notranslate nohighlight">
\[
\Large p_+ = P\left(y_i = 1 \mid \textbf{x}_\text{i}, \textbf{w}\right)
\]</div>
<p>Being able to predict not just a response ( “+1” or “-1”) but the <em>probability</em> of assignment to class “+1” is a very important requirement in many business problems e.g. credit scoring where logistic regression is traditionally used. Customers who have applied for a loan are ranked based on this predicted probability (in descending order) to obtain a scoreboard that rates customers from bad to good. Below is an example of such a toy scoreboard.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/topic4_toy_scorecard_eng.png"><img alt="../../_images/topic4_toy_scorecard_eng.png" src="../../_images/topic4_toy_scorecard_eng.png" style="width: 480px;" />
</a>
</figure>
<p>The bank chooses a threshold <span class="math notranslate nohighlight">\(p_*\)</span> to predict the probability of loan default (in the picture it’s <span class="math notranslate nohighlight">\(0.15\)</span>) and stops approving loans starting from that value. Moreover, it is possible to multiply this predicted probability by the loan amount to get the expectation of losses from the client, which can also constitute good business metrics (scoring experts may have more to add, but the main gist is this).</p>
<p>To predict the probability <span class="math notranslate nohighlight">\(p_+ \in [0,1]\)</span>, we can start by constructing a linear prediction using OLS: <span class="math notranslate nohighlight">\(b(\textbf{x}) = \textbf{w}^\text{T} \textbf{x} \in \mathbb{R}\)</span>. But converting the resulting value to the probability within in the [0, 1] range requires some function <span class="math notranslate nohighlight">\(f: \mathbb{R} \rightarrow [0,1]\)</span>. Logistic regression uses a specific function for this: <span class="math notranslate nohighlight">\(\sigma(z) = \frac{1}{1 + \exp^{-z}}\)</span>. Now let’s understand what the prerequisites are.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/topic4_sigmoid.png"><img alt="../../_images/topic4_sigmoid.png" src="../../_images/topic4_sigmoid.png" style="width: 480px;" />
</a>
</figure>
<p>Let’s denote the probability of an event <span class="math notranslate nohighlight">\(X\)</span> as <span class="math notranslate nohighlight">\(P(X)\)</span>. Then the odds  <span class="math notranslate nohighlight">\(OR(X)\)</span> are determined by <span class="math notranslate nohighlight">\(\frac{P(X)}{1-P(X)}\)</span>, which is the ratio of the probabilities of whether or not an event will happen. It is obvious that the probability and odds contain the same information, but, while <span class="math notranslate nohighlight">\(P(X)\)</span> ranges from 0 to 1, <span class="math notranslate nohighlight">\(OR(X)\)</span> is in the range of 0 to <span class="math notranslate nohighlight">\(\infty\)</span>.</p>
<p>If we calculate the logarithm of <span class="math notranslate nohighlight">\(OR(X)\)</span> (a logarithm of odds or log probability ratio), it is easy to notice that <span class="math notranslate nohighlight">\(\log{OR(X)} \in \mathbb{R}\)</span>. This is what we will use with OLS.</p>
<p>Let’s see how logistic regression will make a prediction <span class="math notranslate nohighlight">\(p_+ = P\left(y_i = 1 \mid \textbf{x}_\text{i}, \textbf{w}\right)\)</span>. (For now, let’s assume that we have somehow obtained weights <span class="math notranslate nohighlight">\(\textbf{w}\)</span> i.e. trained the model. Later, we’ll look at how it is done.)</p>
<p><strong>Step 1.</strong> Calculate <span class="math notranslate nohighlight">\(w_{0}+w_{1}x_1 + w_{2}x_2 + ... = \textbf{w}^\text{T}\textbf{x}\)</span>. (Equation <span class="math notranslate nohighlight">\(\textbf{w}^\text{T}\textbf{x} = 0\)</span> defines a hyperplane separating the examples into two classes);</p>
<p><strong>Step 2.</strong> Compute the log odds: <span class="math notranslate nohighlight">\( \log(OR_{+}) = \textbf{w}^\text{T}\textbf{x}\)</span>.</p>
<p><strong>Step 3.</strong> Now that we have the chance of assigning an example to the class of “+” - <span class="math notranslate nohighlight">\(OR_{+}\)</span>, calculate <span class="math notranslate nohighlight">\(p_{+}\)</span> using the simple relationship:</p>
<div class="math notranslate nohighlight">
\[\large p_{+} = \frac{OR_{+}}{1 + OR_{+}} = \frac{\exp^{\textbf{w}^\text{T}\textbf{x}}}{1 + \exp^{\textbf{w}^\text{T}\textbf{x}}} = \frac{1}{1 + \exp^{-\textbf{w}^\text{T}\textbf{x}}} = \sigma(\textbf{w}^\text{T}\textbf{x})\]</div>
<p>On the right side, you can see that we have the sigmoid function.</p>
<p>So, logistic regression predicts the probability of assigning an example to the “+” class (assuming that we know the features and weights of the model) as a sigmoid transformation of a linear combination of the weight vector and the feature vector:</p>
<div class="math notranslate nohighlight">
\[\large p_+(\textbf{x}_\text{i}) = P\left(y_i = 1 \mid \textbf{x}_\text{i}, \textbf{w}\right) = \sigma(\textbf{w}^\text{T}\textbf{x}_\text{i}). \]</div>
<p>Next, we will see how the model is trained. We will again rely on maximum likelihood estimation.</p>
</section>
<section id="maximum-likelihood-estimation-and-logistic-regression">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">3. Maximum Likelihood Estimation and Logistic Regression</a><a class="headerlink" href="#maximum-likelihood-estimation-and-logistic-regression" title="Link to this heading">#</a></h2>
<p>Now let’s see how an optimization problem for logistic regression is obtained from the MLE, namely, minimization of the <em>logistic</em> loss function. We have just seen that logistic regression models the probability of assigning an example to the class “+” as:</p>
<div class="math notranslate nohighlight">
\[\Large p_+(\textbf{x}_\text{i}) = P\left(y_i = 1 \mid \textbf{x}_\text{i}, \textbf{w}\right) = \sigma(\textbf{w}^T\textbf{x}_\text{i})\]</div>
<p>Then, for the class “-”, the corresponding expression is as follows:</p>
<div class="math notranslate nohighlight">
\[
\Large p_-(\textbf{x}_\text{i})  = P\left(y_i = -1 \mid \textbf{x}_\text{i}, \textbf{w}\right)  = 1 - \sigma(\textbf{w}^T\textbf{x}_\text{i}) = \sigma(-\textbf{w}^T\textbf{x}_\text{i})
\]</div>
<p>Both of these expressions can be cleverly combined into one (watch carefully, maybe you are being tricked):</p>
<div class="math notranslate nohighlight">
\[\Large P\left(y = y_i \mid \textbf{x}_\text{i}, \textbf{w}\right) = \sigma(y_i\textbf{w}^T\textbf{x}_\text{i})\]</div>
<p>The expression <span class="math notranslate nohighlight">\(M(\textbf{x}_\text{i}) = y_i\textbf{w}^T\textbf{x}_\text{i}\)</span> is known as the margin of classification on the object <span class="math notranslate nohighlight">\(\textbf{x}_\text{i}\)</span> (not to be confused with a gap, which is also called margin, in the SVM context). If it is non-negative, the model is correct in choosing the class of the object <span class="math notranslate nohighlight">\(\textbf{x}_\text{i}\)</span>; if it is negative, then the object <span class="math notranslate nohighlight">\(\textbf{x}_\text{i}\)</span> is misclassified. Note that the margin is defined for objects in the training set only where real target class labels <span class="math notranslate nohighlight">\(y_i\)</span> are known.</p>
<p>To understand exactly why we have come to such a conclusion, let us turn to the geometrical interpretation of the linear classifier.</p>
<p>First, I would recommend looking at a classic, introductory problem in linear algebra: find the distance from the point with a radius-vector <span class="math notranslate nohighlight">\(\textbf{x}_A\)</span> to a plane defined by the equation <span class="math notranslate nohighlight">\(\textbf{w}^\text{T}\textbf{x} = 0.\)</span></p>
<details>
  <summary> Answer </summary>
<div class="math notranslate nohighlight">
\[
\rho(\textbf{x}_A, \textbf{w}^\text{T}\textbf{x} = 0) = \frac{\textbf{w}^\text{T}\textbf{x}_A}{||\textbf{w}||}
\]</div>
</details>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/topic4_simple_linal_task.png"><img alt="../../_images/topic4_simple_linal_task.png" src="../../_images/topic4_simple_linal_task.png" style="width: 480px;" />
</a>
</figure>
<p>When we get to the answer, we will understand that the greater the absolute value of the expression <span class="math notranslate nohighlight">\(\textbf{w}^\text{T}\textbf{x}_\text{i}\)</span>, the farther the point <span class="math notranslate nohighlight">\(\textbf{x}_\text{i}\)</span> is from the plane <span class="math notranslate nohighlight">\(\textbf{w}^\text{T}\textbf{x} = 0.\)</span></p>
<p>Hence, our expression <span class="math notranslate nohighlight">\(M(\textbf{x}_\text{i}) = y_i\textbf{w}^\text{T}\textbf{x}_\text{i}\)</span> is a kind of “confidence” in our model’s classification of the object <span class="math notranslate nohighlight">\(\textbf{x}_\text{i}\)</span>:</p>
<ul class="simple">
<li><p>if the margin is large (in absolute value) and positive, the class label is set correctly, and the object is far away from the separating hyperplane i.e. classified confidently. See Point <span class="math notranslate nohighlight">\(x_3\)</span> on the picture;</p></li>
<li><p>if the margin is large (in absolute value) and negative, then class label is set incorrectly, and the object is far from the separating hyperplane (the object is most likely an anomaly; for example, it could be improperly labeled in the training set). See Point <span class="math notranslate nohighlight">\(x_1\)</span> on the picture;</p></li>
<li><p>if the margin is small (in absolute value), then the object is close to the separating hyperplane, and the margin sign determines whether the object is correctly classified. See Points <span class="math notranslate nohighlight">\(x_2\)</span> and <span class="math notranslate nohighlight">\(x_4\)</span> on the plot;</p></li>
</ul>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/topic4_margin.png"><img alt="../../_images/topic4_margin.png" src="../../_images/topic4_margin.png" style="width: 480px;" />
</a>
</figure>
<p>Let’s now compute the likelihood of the data set i.e. the probability of observing the given vector <span class="math notranslate nohighlight">\(\textbf{y}\)</span> from data set <span class="math notranslate nohighlight">\(X\)</span>. We’ll make a strong assumption: objects come independently from one distribution (<em>i.i.d.</em>). Then, we can write</p>
<div class="math notranslate nohighlight">
\[\Large P\left(\textbf{y} \mid \textbf{X}, \textbf{w}\right) = \prod_{i=1}^{\ell} P\left(y = y_i \mid \textbf{x}_\text{i}, \textbf{w}\right),\]</div>
<p>where <span class="math notranslate nohighlight">\(\ell\)</span> is the length of data set <span class="math notranslate nohighlight">\(\textbf{X}\)</span> (number of rows).</p>
<p>As usual, let’s take the logarithm of this expression because a sum is much easier to optimize than the product:</p>
<div class="math notranslate nohighlight">
\[\Large \log P\left(\textbf{y} \mid \textbf{X}, \textbf{w}\right) = \log \prod_{i=1}^{\ell} P\left(y = y_i \mid \textbf{x}_\text{i}, \textbf{w}\right) = \log \prod_{i=1}^{\ell} \sigma(y_i\textbf{w}^\text{T}\textbf{x}_\text{i})   = \]</div>
<div class="math notranslate nohighlight">
\[\Large  = \sum_{i=1}^{\ell} \log \sigma(y_i\textbf{w}^\text{T}\textbf{x}_\text{i}) = \sum_{i=1}^{\ell} \log \frac{1}{1 + \exp^{-y_i\textbf{w}^\text{T}\textbf{x}_\text{i}}} = - \sum_{i=1}^{\ell} \log (1 + \exp^{-y_i\textbf{w}^\text{T}\textbf{x}_\text{i}})\]</div>
<p>Maximizing the likelihood is equivalent to minimizing the expression:</p>
<div class="math notranslate nohighlight">
\[\Large \mathcal{L_{\log}} (\textbf X, \textbf{y}, \textbf{w}) = \sum_{i=1}^{\ell} \log (1 + \exp^{-y_i\textbf{w}^\text{T}\textbf{x}_\text{i}}).\]</div>
<p>This is <em>logistic</em> loss function that is summed over all objects in the training set.</p>
<p>Let’s look at the new function as a function of margin <span class="math notranslate nohighlight">\(L(M) = \log (1 + \exp^{-M})\)</span> and plot it along with <em>zero-one loss</em> graph, which simply penalizes the model for error on each object by 1 (negative margin): <span class="math notranslate nohighlight">\(L_{1/0}(M) = [M &lt; 0]\)</span>.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/topic4_logloss_margin_eng.png"><img alt="../../_images/topic4_logloss_margin_eng.png" src="../../_images/topic4_logloss_margin_eng.png" style="width: 480px;" />
</a>
</figure>
<p>The picture reflects the idea that, if we are not able to directly minimize the number of errors in the classification problem (at least not by gradient methods - derivative of the zero-one loss function at zero turns to infinity), we can minimize its upper bounds. For the logistic loss function (where the logarithm is binary, but this does not matter), the following is valid:</p>
<div class="math notranslate nohighlight">
\[\Large \mathcal{L_{1/0}} (\textbf X, \textbf{y}, \textbf{w}) = \sum_{i=1}^{\ell} [M(\textbf{x}_\text{i}) &lt; 0] \leq \sum_{i=1}^{\ell} \log (1 + \exp^{-y_i\textbf{w}^\text{T}\textbf{x}_\text{i}}) = \mathcal{L_{\log}} (\textbf X, \textbf{y}, \textbf{w}), \]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{L_{1/0}} (\textbf X, \textbf{y})\)</span> is simply the number of errors of logistic regression with weights <span class="math notranslate nohighlight">\(\textbf{w}\)</span> on a data set <span class="math notranslate nohighlight">\((\textbf X, \textbf{y})\)</span>.</p>
<p>Thus, by reducing the upper bound of <span class="math notranslate nohighlight">\(\mathcal{L_{log}}\)</span> by the number of classification errors, we hope to reduce the number of errors itself.</p>
</section>
<section id="l-2-regularization-of-logistic-loss">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">4. <span class="math notranslate nohighlight">\(L_2\)</span>-Regularization of Logistic Loss</a><a class="headerlink" href="#l-2-regularization-of-logistic-loss" title="Link to this heading">#</a></h2>
<p><span class="math notranslate nohighlight">\(L_2\)</span>-regularization of logistic regression is almost the same as in the case of ridge regression. Instead of minimizing the function <span class="math notranslate nohighlight">\(\mathcal{L_{\log}} (\textbf X, \textbf{y}, \textbf{w})\)</span> we minimize the following:</p>
<div class="math notranslate nohighlight">
\[\Large \mathcal{J}(\textbf X, \textbf{y}, \textbf{w}) = \mathcal{L_{\log}} (\textbf X, \textbf{y}, \textbf{w}) + \lambda |\textbf{w}|^2\]</div>
<p>In the case of logistic regression, a reverse regularization coefficient <span class="math notranslate nohighlight">\(C = \frac{1}{\lambda}\)</span> is typically introduced. Then the solution to the problem would be:</p>
<div class="math notranslate nohighlight">
\[\Large \widehat{\textbf w}  = \arg \min_{\textbf{w}} \mathcal{J}(\textbf X, \textbf{y}, \textbf{w}) =  \arg \min_{\textbf{w}}\ (C\sum_{i=1}^{\ell} \log (1 + \exp^{-y_i\textbf{w}^\text{T}\textbf{x}_\text{i}})+ |\textbf{w}|^2)\]</div>
<p>Next, we’ll look at an example that allows us to intuitively understand one of the interpretations of regularization.</p>
</section>
<section id="useful-resources">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">5. Useful resources</a><a class="headerlink" href="#useful-resources" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Medium <a class="reference external" href="https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220">“story”</a> based on this notebook</p></li>
<li><p>Main course <a class="reference external" href="https://mlcourse.ai">site</a>, <a class="reference external" href="https://github.com/Yorko/mlcourse.ai">course repo</a>, and YouTube <a class="reference external" href="https://www.youtube.com/watch?v=QKTuw4PNOsU&amp;amp;list=PLVlY_7IJCMJeRfZ68eVfEcu-UcN9BbwiX">channel</a></p></li>
<li><p>Course materials as a <a class="reference external" href="https://www.kaggle.com/kashnitsky/mlcourse">Kaggle Dataset</a></p></li>
<li><p>If you read Russian: an <a class="reference external" href="https://habrahabr.ru/company/ods/blog/323890/">article</a> on <a class="reference external" href="http://Habr.com">Habr.com</a> with ~ the same material. And a <a class="reference external" href="https://youtu.be/oTXGQ-_oqvI">lecture</a> on YouTube</p></li>
<li><p>A nice and concise overview of linear models is given in the book <a class="reference external" href="http://www.deeplearningbook.org">“Deep Learning”</a> (I. Goodfellow, Y. Bengio, and A. Courville).</p></li>
<li><p>Linear models are covered practically in every ML book. We recommend “Pattern Recognition and Machine Learning” (C. Bishop) and “Machine Learning: A Probabilistic Perspective” (K. Murphy).</p></li>
<li><p>If you prefer a thorough overview of linear model from a statistician’s viewpoint, then look at “The elements of statistical learning” (T. Hastie, R. Tibshirani, and J. Friedman).</p></li>
<li><p>The book “Machine Learning in Action” (P. Harrington) will walk you through implementations of classic ML algorithms in pure Python.</p></li>
<li><p><a class="reference external" href="http://scikit-learn.org/stable/documentation.html">Scikit-learn</a> library. These guys work hard on writing really clear documentation.</p></li>
<li><p>Scipy 2017 <a class="reference external" href="https://github.com/amueller/scipy-2017-sklearn">scikit-learn tutorial</a> by Alex Gramfort and Andreas Mueller.</p></li>
<li><p>One more <a class="reference external" href="https://github.com/diefimov/MTH594_MachineLearning">ML course</a> with very good materials.</p></li>
<li><p><a class="reference external" href="https://github.com/rushter/MLAlgorithms">Implementations</a> of many ML algorithms. Search for linear regression and logistic regression.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./book/topic04"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="topic4_linear_models_part1_mse_likelihood_bias_variance.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Topic 4. Linear Classification and Regression</p>
      </div>
    </a>
    <a class="right-next"
       href="topic4_linear_models_part3_regul_example.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Topic 4. Linear Classification and Regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-linear-classification">Part 2. Linear Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#article-outline">Article outline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-classifier">1. Linear Classifier</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-as-a-linear-classifier">2. Logistic Regression as a Linear Classifier</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-and-logistic-regression">3. Maximum Likelihood Estimation and Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-2-regularization-of-logistic-loss">4. <span class="math notranslate nohighlight">\(L_2\)</span>-Regularization of Logistic Loss</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#useful-resources">5. Useful resources</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yury Kashnitsky (yorko)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>